{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec trained on \"Game of Thrones\" texts\n",
    "\n",
    "In this example, we train word embeddings using the texts of \"Game of Thrones\" five books. The point is to show how word embeddings can be trained from plain texts. The raw text is first preprocessed, removing english stop words, punctuation and other special symbols. Only text words that are longer than one character are retained and used. The text is also tokenized in sentences and words.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# to ensure reproducibility of results depending on random factors\n",
    "sd = 7 ; np.random.seed(sd) ; random.seed(sd) ; os.environ['PYTHONHASHSEED'] = str(sd)\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def get_words(txt):\n",
    "    return list(filter(\n",
    "        lambda x: x not in STOP_WORDS, # remove english stopwords \n",
    "        re.findall(r'\\b(\\w+)\\b', txt)  # keep only text words\n",
    "    ))\n",
    "\n",
    "# Returns a list of a list of words. Each sublist is a sentence.\n",
    "def parse_sentence_words(input_file_names):\n",
    "    sentence_words = []\n",
    "    for file_name in input_file_names:\n",
    "        f = open(file_name, encoding='utf-8', errors='ignore') ; line_lst = f.readlines() # open file and read lines\n",
    "        for line in line_lst:\n",
    "            line = line.strip().lower()  # strips off trailing and ending and lowercases\n",
    "            line = line.encode('ascii','ignore').decode('unicode_escape') # remove non-standard symbols\n",
    "            sent_words = map(get_words, sent_tokenize(line)) # get only words\n",
    "            sent_words = list(filter(lambda sw: len(sw) > 1, sent_words)) # remove single letters\n",
    "            if len(sent_words) > 1:\n",
    "                sentence_words += sent_words # add words that are longer than one character\n",
    "        f.close()  # close file \n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec Model\n",
    "Word2vec method is used to produce the word embeddings. More specifically, we use the Gensim implementation in Pythnon of word2vec method. Vectors of 300 dimensions are produced for each word, using a context windows of 4 words and a minimal count of 3 for each text word. The training runs fast since it is computed in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the five text files of each book\n",
    "input_file_names = [\"001ssb.txt\", \"002ssb.txt\", \"003ssb.txt\", \n",
    "                    \"004ssb.txt\", \"005ssb.txt\"]\n",
    "\n",
    "# the entire list of sentence words\n",
    "GOT_SENTENCE_WORDS = parse_sentence_words(input_file_names)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# vector_size: the dimensionality of the embedding vectors.\n",
    "# window: the maximum distance between the current and predicted word within a sentence.\n",
    "model = Word2Vec(GOT_SENTENCE_WORDS, vector_size=500, window=10, min_count=3, workers=28, sg=1)\n",
    "model.wv.save_word2vec_format(\"got_word2vec.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Similarities\n",
    "\n",
    "Here we show a simple and funny utilization of the learned word embeddings. For a specific word we pick, it is possible to list the N most similar (e.g., the top 15) words with it and the values of the respective vectors. In this example, we can list words (which can be portratizations) that are similar with the main characters in the books.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jaime', 0.8315284848213196),\n",
       " ('margaery', 0.8177911639213562),\n",
       " ('dwarf', 0.8108684420585632),\n",
       " ('shae', 0.8023456335067749),\n",
       " ('joffrey', 0.8001843690872192),\n",
       " ('arianne', 0.7980315685272217),\n",
       " ('kevan', 0.7944758534431458),\n",
       " ('imp', 0.7912792563438416),\n",
       " ('lancel', 0.7868460416793823),\n",
       " ('varys', 0.7624967098236084),\n",
       " ('littlefinger', 0.7598744630813599),\n",
       " ('dontos', 0.7597967982292175),\n",
       " ('daario', 0.7585698962211609),\n",
       " ('bronn', 0.7570660710334778),\n",
       " ('joff', 0.755999743938446)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### finding similar words with a certain word\n",
    "# model.wv.most_similar('king', topn=15)\n",
    "# model.wv.most_similar('queen', topn=15)\n",
    "model.wv.most_similar('cersei', topn=15)\n",
    "# model.wv.most_similar('margaery', topn=15)\n",
    "# model.wv.most_similar('joffrey', topn=15)\n",
    "# model.wv.most_similar('ned', topn=15)\n",
    "# model.wv.most_similar('jaime', topn=15)\n",
    "# model.wv.most_similar('stannis', topn=15)\n",
    "# model.wv.most_similar('renly', topn=15)\n",
    "# model.wv.most_similar('aerys', topn=15)\n",
    "# model.wv.most_similar('sansa', topn=15)\n",
    "# model.wv.most_similar('tyrion', topn=15)\n",
    "# model.wv.most_similar('ramsay', topn=15)\n",
    "# model.wv.most_similar('theon', topn=15)\n",
    "# model.wv.most_similar('brienne', topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "ninhu"
   }
  ],
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
